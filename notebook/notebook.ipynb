{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ML_Models_with_Results.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbBXWZpcedCS"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8b4Vw6bzsIY"
      },
      "source": [
        "# Import Dependencies\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import random_split\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "from IPython.display import clear_output "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB8bSF5VG39l"
      },
      "source": [
        "# Classification Net\n",
        "import torchvision.models as models\n",
        "resnet18 = models.resnet18(pretrained=True) #output in 1x1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8lcbwLOBx5t"
      },
      "source": [
        "class Submodel_1(nn.Module):\n",
        "    def __init__(self,model):\n",
        "        super(Submodel_1, self).__init__()\n",
        "        image_modules = list(model.children())[:-5] #all layer expect last five layers\n",
        "        self.modelA = nn.Sequential(*image_modules)\n",
        "        \n",
        "    def forward(self, image):\n",
        "        a = self.modelA(image)\n",
        "        x = F.sigmoid(a)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpu_s-o_wvnZ"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self,num_in_features):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.name = 'classifier'\n",
        "        self.num_in_features = num_in_features\n",
        "        self.fc1 = nn.Linear(self.num_in_features, 5000)\n",
        "        self.fc2 = nn.Linear(5000, 320)\n",
        "        self.fc3 = nn.Linear(320, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.num_in_features)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        X = F.sigmoid(x)\n",
        "        x = x.squeeze(1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un6AeuFiArDJ"
      },
      "source": [
        "# Load Classifiction data\n",
        "\n",
        "data_path_classify = \"/content/drive/My Drive/Project Files/Final_Dataset/final_dataset/classification_data\"\n",
        "data_path_rndForest = \"/content/drive/My Drive/Project Files/Final_Dataset/final_dataset/classification_data\"\n",
        "\n",
        "\n",
        "transform_classify = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "transform_rndForest = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "def ignore_noncovid(path):\n",
        "    if path.find(\"non_covid\") == -1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def ignore_nii(path):\n",
        "    if path.find(\"covid_mask_png\") == -1 or path.find(\"outputFile.csv\") != -1:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "data_classify = torchvision.datasets.ImageFolder(root=data_path_classify,transform=transform_classify)\n",
        "data_rndForest = torchvision.datasets.ImageFolder(root=data_path_rndForest,transform=transform_rndForest)\n",
        "\n",
        "\n",
        "# Calculate split lengths\n",
        "total_size_classify = len(data_classify)\n",
        "train_size_classify = round(0.7*total_size_classify)\n",
        "valid_size_classify = round(0.15*total_size_classify)\n",
        "test_size_classify = round(0.15*total_size_classify)\n",
        "\n",
        "# Seperate into Train, Val and Test sets\n",
        "random.seed(14)\n",
        "train_set_classify, valid_set_classify, test_set_classify = torch.utils.data.random_split(data_classify, [train_size_classify,valid_size_classify,test_size_classify])\n",
        "\n",
        "# Calculate split lengths\n",
        "total_size_rndForest = len(data_rndForest)\n",
        "train_size_rndForest = round(0.7*total_size_rndForest)\n",
        "test_size_rndForest = round(0.3*total_size_rndForest)\n",
        "\n",
        "train_set_rndForest, test_set_rndForest = torch.utils.data.random_split(data_rndForest, [train_size_rndForest,test_size_rndForest] )\n",
        "\n",
        "\n",
        "# Seperate random Forest Data into images and labels\n",
        "train_set_imgs_rndForest = []\n",
        "train_set_labels_rndForest = []\n",
        "test_set_imgs_rndForest = []\n",
        "test_set_labels_rndForest = []\n",
        "count = 1\n",
        "for data in train_set_rndForest:\n",
        "    clear_output(wait=True)\n",
        "    print(\"Current random Forest split progress:\", count, \"/\",len(data_rndForest))\n",
        "    train_set_imgs_rndForest.append(data[0])\n",
        "    train_set_labels_rndForest.append(data[1])\n",
        "    count += 1\n",
        "\n",
        "for data in test_set_rndForest:\n",
        "    clear_output(wait=True)\n",
        "    print(\"Current random Forest split progress:\", count, \"/\",len(data_rndForest))\n",
        "    test_set_imgs_rndForest.append(data[0])\n",
        "    test_set_labels_rndForest.append(data[1])\n",
        "    count += 1\n",
        "\n",
        "train_set_imgs_rndForest = torch.stack(train_set_imgs_rndForest)\n",
        "train_set_labels_rndForest = np.array(train_set_labels_rndForest)\n",
        "test_set_imgs_rndForest = torch.stack(test_set_imgs_rndForest)\n",
        "test_set_labels_rndForest = np.array(test_set_labels_rndForest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gHZAwcO4R3A"
      },
      "source": [
        "# Baseline Model\n",
        "# 1-Random Forest (From Scikit Learn)\n",
        "baseLine_rndForest = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "# Train randomForest\n",
        "with torch.no_grad():\n",
        "    trainFeature = resnet18(train_set_imgs_rndForest)\n",
        "    testFeature = resnet18(test_set_imgs_rndForest)\n",
        "\n",
        "baseLine_rndForest.fit(trainFeature, train_set_labels_rndForest)\n",
        "\n",
        "# Test randomForest Accuracy\n",
        "pred = baseLine_rndForest.predict(testFeature)\n",
        "correct = (pred == test_set_labels_rndForest).sum()\n",
        "total = testFeature.shape[0]\n",
        "print(\"Random Forest Accuracy is: \", correct/total*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ5_4E2uCGMw"
      },
      "source": [
        "def train_net(transfer_net,net, train_data,val_data, batch_size=64, learning_rate=0.01, num_epochs=30, gpu = True):\n",
        "    ########################################################################\n",
        "    # Fixed PyTorch random seed for reproducible result\n",
        "    torch.manual_seed(1000)\n",
        "    ########################################################################\n",
        "    # Define the Loss function and optimizer\n",
        "    # The loss function will be Binary Cross Entropy (BCE). In this case we\n",
        "    # will use the BCEWithLogitsLoss which takes unnormalized output from\n",
        "    # the neural network and scalar label.\n",
        "    # Optimizer will be SGD with Momentum.\n",
        "    learned_parameters = []\n",
        "    for param in transfer_net.parameters():\n",
        "        learned_parameters.append(param)\n",
        "    for param in net.parameters():\n",
        "        learned_parameters.append(param)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(learned_parameters, lr=learning_rate)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data,batch_size=batch_size,shuffle=True)\n",
        "    ########################################################################\n",
        "    # Set up some numpy arrays to store the training/test loss/erruracy\n",
        "    train_err = np.zeros(num_epochs)\n",
        "    train_loss = np.zeros(num_epochs)\n",
        "    val_err = np.zeros(num_epochs)\n",
        "    val_loss = np.zeros(num_epochs)\n",
        "    ########################################################################\n",
        "    # Train the network\n",
        "    # Loop over the data iterator and sample a new batch of training data\n",
        "    # Get the output from the network, and optimize our loss function.\n",
        "    start_time = time.time()\n",
        "    if gpu:\n",
        "        transfer_net = transfer_net.cuda()\n",
        "        net = net.cuda()\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "        total_train_loss = 0.0\n",
        "        total_train_err = 0.0\n",
        "        total_epoch = 0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # Get the inputs\n",
        "            inputs, labels = data\n",
        "            if gpu:\n",
        "              inputs = inputs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass, backward pass, and optimize\n",
        "            outputs = net(transfer_net(inputs))\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Calculate the statistics\n",
        "            corr = (outputs > 0.0).squeeze().long() != labels\n",
        "            total_train_err += int(corr.sum())\n",
        "            total_train_loss += loss.item()\n",
        "            total_epoch += len(labels)\n",
        "        train_err[epoch] = float(total_train_err) / total_epoch\n",
        "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
        "        val_err[epoch], val_loss[epoch] = evaluate(transfer_net,net, val_loader, criterion,gpu)\n",
        "        print((\"Epoch {}: Train err: {}, Train loss: {} |\"+\n",
        "               \"Validation err: {}, Validation loss: {}\").format(\n",
        "                   epoch + 1,\n",
        "                   train_err[epoch],\n",
        "                   train_loss[epoch],\n",
        "                   val_err[epoch],\n",
        "                   val_loss[epoch]))\n",
        "        # Save the current model (checkpoint) to a file\n",
        "        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
        "        torch.save(net.state_dict(), model_path)\n",
        "    print('Finished Training')\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "    # Write the train/test loss/err into CSV file for plotting later\n",
        "    epochs = np.arange(1, num_epochs + 1)\n",
        "    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
        "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
        "    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
        "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)\n",
        "\n",
        "\n",
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing the hyperparameters\n",
        "    Returns:\n",
        "        path: A string with the hyperparameter name and value concatenated\n",
        "    \"\"\"\n",
        "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
        "                                                   batch_size,\n",
        "                                                   learning_rate,\n",
        "                                                   epoch)\n",
        "    return path\n",
        "\n",
        "def evaluate(transfer_net, net, loader, criterion, gpu):\n",
        "    \"\"\" Evaluate the network on the validation set.\n",
        "\n",
        "     Args:\n",
        "         net: PyTorch neural network object\n",
        "         loader: PyTorch data loader for the validation set\n",
        "         criterion: The loss function\n",
        "     Returns:\n",
        "         err: A scalar for the avg classification error over the validation set\n",
        "         loss: A scalar for the average loss function over the validation set\n",
        "     \"\"\"\n",
        "    total_loss = 0.0\n",
        "    total_err = 0.0\n",
        "    total_epoch = 0\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        inputs, labels = data\n",
        "        if gpu:\n",
        "          inputs = inputs.cuda()\n",
        "          labels = labels.cuda()\n",
        "        outputs = net(transfer_net(inputs))\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        corr = (outputs > 0.0).squeeze().long() != labels\n",
        "        total_err += int(corr.sum())\n",
        "        total_loss += loss.item()\n",
        "        total_epoch += len(labels)\n",
        "    err = float(total_err) / total_epoch\n",
        "    loss = float(total_loss) / (i + 1)\n",
        "    return err, loss\n",
        "\n",
        "###############################################################################\n",
        "# Training Curve\n",
        "def plot_training_curve(path):\n",
        "    \"\"\" Plots the training curve for a model run, given the csv files\n",
        "    containing the train/validation error/loss.\n",
        "\n",
        "    Args:\n",
        "        path: The base path of the csv files produced during training\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
        "    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
        "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
        "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
        "    plt.title(\"Train vs Validation Error\")\n",
        "    n = len(train_err) # number of epochs\n",
        "    plt.plot(range(1,n+1), train_err, label=\"Train\")\n",
        "    plt.plot(range(1,n+1), val_err, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Error\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    plt.title(\"Train vs Validation Loss\")\n",
        "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
        "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4spne0RwdHv"
      },
      "source": [
        "# Initialize handcrafted classifier and train the transfer learning + ANN network\n",
        "classifier = Classifier(1000)\n",
        "train_net(resnet18,classifier,train_set_classify,valid_set_classify,batch_size=64,learning_rate=0.001,num_epochs=15)\n",
        "model_path = get_model_name(\"classifier\", batch_size=64, learning_rate=0.001, epoch=14)\n",
        "plot_training_curve(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez6I7O-t_64F"
      },
      "source": [
        "# Report the Test Accuracy\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "gpu = True\n",
        "test_err, test_acc = evaluate(resnet18,classifier,torch.utils.data.DataLoader(test_set_classify,batch_size=64),criterion,gpu)\n",
        "print('Test Accuracy is',1-test_err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUMsBFmzix3b"
      },
      "source": [
        "data_path_img_autoEncoder = \"/content/drive/My Drive/Project Files/Final_Dataset/final_dataset/classification_data\"\n",
        "data_path_mask_autoEncoder = \"/content/drive/My Drive/Project Files/Final_Dataset/final_dataset/covid_mask\"\n",
        "\n",
        "transform_autoEncoder = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "transform_feature_extractor = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "data_img_feature_extractor = torchvision.datasets.ImageFolder(root=data_path_img_autoEncoder,transform=transform_feature_extractor,is_valid_file=ignore_noncovid)\n",
        "data_img_autoEncoder = torchvision.datasets.ImageFolder(root=data_path_img_autoEncoder,transform=transform_autoEncoder,is_valid_file=ignore_noncovid)\n",
        "data_mask_autoEncoder = torchvision.datasets.ImageFolder(root=data_path_mask_autoEncoder,transform=transform_autoEncoder,is_valid_file=ignore_nii)\n",
        "\n",
        "print('the number of images match=',len(data_img_autoEncoder) == len(data_mask_autoEncoder))\n",
        "\n",
        "# Build (image,Mask) pairs\n",
        "data_autoEncoder = []\n",
        "for i in range(len(data_img_autoEncoder)):\n",
        "\n",
        "  data_autoEncoder.append((data_img_autoEncoder[i][0],data_mask_autoEncoder[i][0],data_img_feature_extractor[i][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4AHsmoe1xcE"
      },
      "source": [
        "# Training:Validation:Test = 0.7:0.15:0.15\n",
        "random.seed(14)\n",
        "random.shuffle(data_autoEncoder)\n",
        "\n",
        "train_index = int(len(data_autoEncoder) * 0.7)\n",
        "val_index = int(len(data_autoEncoder) * 0.85)\n",
        "\n",
        "training_data = data_autoEncoder[:train_index]\n",
        "valid_data = data_autoEncoder[train_index:val_index]\n",
        "test_data = data_autoEncoder[val_index:]\n",
        "print(\"# Train Set: \" + str(len(training_data)))\n",
        "print(\"# Test Set: \" + str(len(test_data)))\n",
        "print(\"# Val Set: \" + str(len(valid_data)))\n",
        "\n",
        "def initialize_loader(train_dataset,valid_dataset,train_batch_size=64, val_batch_size=64):\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=val_batch_size,shuffle=True)\n",
        "    return train_loader, valid_loader\n",
        "train_d = torch.utils.data.DataLoader(training_data, batch_size=64, num_workers=1)\n",
        "valid_d = torch.utils.data.DataLoader(valid_data, batch_size=64, num_workers=1)\n",
        "test_d = torch.utils.data.DataLoader(test_data, batch_size=64, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uroSJdCy15Id"
      },
      "source": [
        "# 2-Convolutional Autoencoder (Hard-coded)\n",
        "class baseline_autoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(baseline_autoEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential( # like the Composition layer you built\n",
        "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 7),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 7),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 7),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 7),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 7),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 7),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 2, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Unet\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, num_filters, num_colours, num_in_channels, kernel=3):\n",
        "        super(UNet, self).__init__()\n",
        "        # Calculate padding\n",
        "        padding = kernel // 2\n",
        "        # Model\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(num_in_channels, num_filters, kernel_size=kernel, padding=padding),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU(),\n",
        "            )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, padding=padding),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.ReLU(),\n",
        "            )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters*2 + 64, num_filters*2, kernel_size=kernel, padding=padding),\n",
        "            nn.BatchNorm2d(num_filters*2),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters*2+num_filters*2, num_filters, kernel_size=kernel, padding=padding),\n",
        "            nn.Upsample(scale_factor=2),            \n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU(),\n",
        "            )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters+num_filters, num_colours, kernel_size=kernel, padding=padding),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.BatchNorm2d(num_colours),\n",
        "            nn.ReLU(),\n",
        "            )\n",
        "        \n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(num_colours+num_in_channels, num_colours, kernel_size=kernel, padding=padding),\n",
        "        )  \n",
        "\n",
        "\n",
        "    def forward(self, x, feature_tensor): \n",
        "        self.o1 = self.layer1(x)\n",
        "        self.o2 = self.layer2(self.o1)\n",
        "        self.o3 = self.layer3(torch.cat((self.o2,feature_tensor),1))\n",
        "        self.o4 = self.layer4(torch.cat((self.o3, self.o2),1))\n",
        "        self.o5 = self.layer5(torch.cat((self.o4, self.o1),1))\n",
        "        self.o6 = self.layer6(torch.cat((self.o5,x),1))\n",
        "        return self.o6\n",
        "\n",
        "\n",
        "def iou_pytorch(outputs, labels):\n",
        "    \n",
        "    SMOOTH = 1e-6\n",
        "    # print('raw',outputs.shape)\n",
        "    outputs = torch.argmax(outputs, 1)\n",
        "    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
        "    # print('output=',outputs)\n",
        "    \n",
        "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
        "    union = (outputs | labels).float().sum((1, 2))         # Will be zero if both are 0\n",
        "    # print('intersection=',intersection)\n",
        "    # print('union=',union)\n",
        "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # smooth our devision to avoid 0/0\n",
        "    # print('iou=',iou)\n",
        "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  \n",
        "    \n",
        "    return thresholded.mean() \n",
        "\n",
        "\n",
        "def convert_to_binary(masks, thres=0.5):\n",
        "    binary_masks = ((masks[:, 0, :, :] ==  128) & (masks[:, 1, :, :] == 0) & (masks[:, 2, :, :] == 0)) + 0.\n",
        "    return binary_masks.long()\n",
        "\n",
        "def run_validation_step(args, epoch, model, loader, feature_extractor):\n",
        "\n",
        "    model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
        "\n",
        "    losses = []\n",
        "    ious = []\n",
        "    with torch.no_grad():\n",
        "        for i, (images, masks,raw_input) in enumerate(loader):\n",
        "            if args.gpu:\n",
        "                images = images.cuda()\n",
        "                masks = masks.cuda()\n",
        "                raw_input = raw_input.cuda()\n",
        "            feature = feature_extractor(raw_input)\n",
        "            output = model(images.float(),feature)\n",
        "            # pred_seg_masks = output[\"out\"]\n",
        "\n",
        "            output_predictions = output.argmax(0)\n",
        "            loss = compute_loss(output, masks.squeeze(1).long())\n",
        "            iou = iou_pytorch(output, masks.squeeze(1).long())\n",
        "            losses.append(loss.data.item())\n",
        "            ious.append(iou.data.item())\n",
        "\n",
        "        val_loss = np.mean(losses)\n",
        "        val_iou = np.mean(ious)\n",
        "    \n",
        "    return val_loss, val_iou\n",
        "\n",
        "def train(args, model, feature_extractor):\n",
        "    \n",
        "    # Set the maximum number of threads to prevent crash\n",
        "    torch.set_num_threads(5)\n",
        "    # Numpy random seed\n",
        "    np.random.seed(args.seed)\n",
        "    \n",
        "    # Save directory\n",
        "    # Create the outputs folder if not created already\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    # Adam Optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learn_rate)\n",
        "\n",
        "    train_loader, valid_loader = initialize_loader(training_data,valid_data,args.train_batch_size,args.val_batch_size)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu: \n",
        "        model.cuda()\n",
        "\n",
        "    start = time.time()\n",
        "    trn_losses = []\n",
        "    val_losses = []\n",
        "    val_ious = []\n",
        "    best_iou = 0.0\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "\n",
        "        # Train the Model\n",
        "        model.train() # Change model to 'train' mode\n",
        "        start_tr = time.time()\n",
        "        \n",
        "        losses = []\n",
        "        for i, (images, masks, raw_input) in enumerate(train_loader):\n",
        "\n",
        "            if args.gpu:\n",
        "                images = images.cuda()\n",
        "                masks = masks.cuda()\n",
        "                raw_input = raw_input.cuda()\n",
        "            features = feature_extractor(raw_input)\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            output = model(images.float(),features)\n",
        "            # pred_seg_masks = output[\"out\"])\n",
        "            # _, pred_labels = torch.max(output, 1, keepdim=True)\n",
        "            loss = compute_loss(output, masks.squeeze(1).long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.data.item())\n",
        "\n",
        "\n",
        "        # plot training images\n",
        "        trn_loss = np.mean(losses)\n",
        "        trn_losses.append(trn_loss)\n",
        "        time_elapsed = time.time() - start_tr\n",
        "        print('Epoch [%d/%d], Loss: %.4f, Time (s): %d' % (\n",
        "                epoch+1, args.epochs, trn_loss, time_elapsed))\n",
        "\n",
        "        # Evaluate the model\n",
        "        start_val = time.time()\n",
        "        val_loss, val_iou = run_validation_step(args, \n",
        "                                                epoch, \n",
        "                                                model,\n",
        "                                                valid_loader, feature_extractor)\n",
        "\n",
        "        if val_iou > best_iou:\n",
        "            best_iou = val_iou\n",
        "            torch.save(model.state_dict(), os.path.join(save_dir, args.checkpoint_name + '-best.ckpt'))\n",
        "\n",
        "        time_elapsed = time.time() - start_val\n",
        "        print('Epoch [%d/%d], Loss: %.4f, mIOU: %.4f, Validation time (s): %d' % (\n",
        "                epoch+1, args.epochs, val_loss, val_iou, time_elapsed))\n",
        "        \n",
        "        val_losses.append(val_loss)\n",
        "        val_ious.append(val_iou)\n",
        "\n",
        "    # Plot training curve\n",
        "    plt.figure()\n",
        "    # plt.plot(trn_losses, \"ro-\", label=\"Train\")\n",
        "    # plt.plot(val_losses, \"go-\", label=\"Validation\")\n",
        "    plt.plot(trn_losses,  label=\"Train\")\n",
        "    plt.plot(val_losses,  label=\"Validation\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.savefig(save_dir+\"/training_curve.png\")\n",
        "\n",
        "    # Plot validation iou curve\n",
        "    plt.figure()\n",
        "    plt.plot(val_ious, \"ro-\", label=\"mIOU\")\n",
        "    plt.legend()\n",
        "    plt.title(\"mIOU\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.savefig(save_dir+\"/val_iou_curve.png\")\n",
        "\n",
        "    print('Saving model...')\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, args.checkpoint_name + '-{}-last.ckpt'.format(args.epochs)))\n",
        "\n",
        "    print('Best model achieves mIOU: %.4f' % best_iou)\n",
        "\n",
        "\n",
        "def compute_loss(pred, gt):\n",
        "    loss = F.cross_entropy(pred, gt,weight=torch.Tensor([0.1,4.15]).cuda())\n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nAbv9Lk2AFy"
      },
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "# Baseline Hyperparameters\n",
        "args_baseline = AttrDict()\n",
        "args_dict = {\n",
        "              'gpu':True, \n",
        "              'checkpoint_name':\"baseline_segmentation\", \n",
        "              'learn_rate':0.1, \n",
        "              'train_batch_size':64, \n",
        "              'val_batch_size': 256, \n",
        "              'epochs':10, \n",
        "              'seed':0,\n",
        "              'experiment_name': 'baseline_segmentation',\n",
        "}\n",
        "args_baseline.update(args_dict)\n",
        "\n",
        "# Unet Hyperparameters\n",
        "args_unet = AttrDict()\n",
        "\n",
        "args_dict = {\n",
        "              'gpu':True, \n",
        "              'checkpoint_name':\"unet_segmentation\", \n",
        "              'learn_rate':0.01, \n",
        "              'train_batch_size':128, \n",
        "              'val_batch_size': 256, \n",
        "              'epochs':20, \n",
        "              'seed':14,\n",
        "              'experiment_name': 'unet_segmentation',\n",
        "}\n",
        "args_unet.update(args_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xz4x3Gz2J9X"
      },
      "source": [
        "# Train baseline model\n",
        "baseline_segmentation = baseline_autoEncoder()\n",
        "train(args_baseline,baseline_segmentation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8hZRRbd2O9_"
      },
      "source": [
        "# Train Unet\n",
        "unet = UNet(10,2,1)\n",
        "feature_extractor = feature_extractor.cuda()\n",
        "train(args_unet,unet,feature_extractor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fKUPgPq2Vgr"
      },
      "source": [
        "# Visualize a few predictions in test set\n",
        "feature_extractor = feature_extractor.cuda()\n",
        "for (imgs,masks,raw_input) in test_d:\n",
        "  pred = unet(imgs.float().cuda(),feature_extractor(raw_input.cuda()))\n",
        "  msk = masks\n",
        "  raw = raw_input\n",
        "  pred = torch.argmax(pred, 1)\n",
        "  break\n",
        "for i in range(len(pred)):\n",
        "  fig = plt.figure(figsize=(15,4.5))\n",
        "  plt.title('prediction vs. ground truth vs. input image')\n",
        "  ax = fig.add_subplot(1,3,1)\n",
        "  plt.imshow(pred[i+10].cpu().detach().numpy())\n",
        "  ax = fig.add_subplot(1,3,2)\n",
        "  plt.imshow(msk[i+10].cpu().detach().numpy().squeeze(0))\n",
        "  ax = fig.add_subplot(1,3,3)\n",
        "  plt.imshow(np.transpose(raw[i+10],(1,2,0)))\n",
        "  i += 1\n",
        "  if i > 10:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}